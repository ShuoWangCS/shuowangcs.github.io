<!DOCTYPE html>
<html>
  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="IE=7.0000" http-equiv="X-UA-Compatible">
    <title>Shuo Wang's Homepage</title>
    <meta name="description" content="Shuo Wang, Associate Research Fellow at University of Science and Technology of China. ">
    <meta name="keywords" content="Shuo Wang, USTC, University of Science and Technology of China, Multimodel, LLM.">
    <link rel="stylesheet" type="text/css" href="./files/ShuoWang.css">
    <style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style>
  </head>
  
  <body>
    <div id="content">
        <div id="news">
           <h2>News</h2>
           <div class="news-item">
                <div class="news-date">22 August 2025</div>                
                <div class="easylink">One paper is accepted by PRCV</div>
            </div>
            <div class="news-item">
                <div class="news-date">1 July 2025</div>                
                <div class="easylink">One paper is accepted by ACM MM</div>
            </div>
            <div class="news-item">
                <div class="news-date">28 June 2025</div>                
                <div class="easylink">Two papers are accepted by ICCV</div>
            </div>
            <div class="news-item">
                <div class="news-date">1 April 2025</div>                
                <div class="easylink">Two papers are accepted by NAACL</div>
            </div>

            <div class="news-item">
                <div class="news-date">10 February 2025</div>                
                <div class="easylink">One paper is accepted by TOMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">7 March 2025</div>                
                <div class="easylink">One paper is accepted by TOMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">4 February 2025</div>                
                <div class="easylink">One paper is accepted by ICML</div>
            </div>
            <div class="news-item">
                <div class="news-date">16 January 2025</div>                
                <div class="easylink">One paper is accepted by TCSVT</div>
            </div>
            <div class="news-item">
                <div class="news-date">1 January 2025</div>                
                <div class="easylink">One paper is accepted by ICASSP</div>
            </div>
            <div class="news-item">
                <div class="news-date">24 December 2024</div>                
                <div class="easylink">One paper is accepted by TMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 November 2024</div>                
                <div class="easylink">One paper is accepted by TOMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">25 October 2024</div>                
                <div class="easylink">One paper is accepted by NeurIPS (Spotlight)</div>
            </div>

            <div class="news-item">
                <div class="news-date">15 October 2024</div>                
                <div class="easylink">One paper is accepted by MMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">14 October 2024</div>                
                <div class="easylink">One paper is accepted by PRCV</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 September 2024</div>                
                <div class="easylink">One paper is accepted by BMCV</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 July 2024</div>                
                <div class="easylink">One paper is accepted by ACM MM</div>
            </div>

            <div class="news-item">
                <div class="news-date">7 July 2024</div>                
                <div class="easylink">One paper is accepted by TIP</div>
            </div>
            <div class="news-item">
                <div class="news-date">30 May 2024</div>                
                <div class="easylink">One paper is accepted by ICMR</div>
            </div>

            <div class="news-item">
                <div class="news-date">15 February 2024</div>                
                <div class="easylink">One paper is accepted by ECCV</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 January 2024</div>                
                <div class="easylink">One paper is accepted by AAAI</div>
            </div>

            <div class="news-item">
                <div class="news-date">15 September 2023</div>                
                <div class="easylink">One paper is accepted by BMVC</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 July 2023</div>                
                <div class="easylink">One paper is accepted by ACM MM</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 June 2023</div>                
                <div class="easylink">One paper is accepted by CVPR</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 January 2023</div>                
                <div class="easylink">One paper is accepted by TOMM</div>
            </div>
            <div class="news-item">
                <div class="news-date">15 November 2022</div>                
                <div class="easylink">One paper is accepted by TIP</div>
            </div>
            <div class="news-item">
                <div class="news-date">10 October 2022</div>                
                <div class="easylink">Three papers are accepted by ACM MM</div>
            </div>
            <div class="news-item">
                <div class="news-date">22 April 2022</div>                
                <div class="easylink">One paper is accepted by TCSVT</div>
            </div>
            <div class="news-item">
                <div class="news-date">25 February 2022</div>                
                <div class="easylink">One paper is accepted by ISEEIE</div>
            </div>
            <div class="news-item">
                <div class="news-date">18 October 2021</div>                
                <div class="easylink">One paper is accepted by Journal of Physics</div>
            </div>
            <div class="news-item">
                <div class="news-date">10 October 2021</div>                
                <div class="easylink">One paper is accepted by PRICAI</div>
            </div>
            <div class="news-item">
                <div class="news-date">3 July 2020</div>                
                <div class="easylink">One paper is accepted by ECCV</div> 
            </div>
            <div class="news-item">
                <div class="news-date">10 May 2019</div>                
                <div class="easylink">One paper is accepted by IJCAI</a></div>
            </div>
            <div class="news-item">
                <div class="news-date">10 September 2019</div>                
                <div class="easylink">One paper is accepted by TOMM</a></div>
            </div>
            <div class="news-item">
                <div class="news-date">10 October 2018</div>                
                <div class="easylink">One paper is accepted by ACM MM</a></div>
            </div>
      </div>

      <div id="left">
        <table style="background-color:white;">
          <tbody>
            <tr nosave="">
              <td valign="CENTER">
                <img src="./images/profile.jpg" height="250" align="left">
              </td>
              <td valign="CENTER" width="2%">
              </td>
              <td valign="CENTER" halign="LEFT">
                <font size="+0">
                  <b>
                    <font size="+2">Shuo Wang&nbsp;</font>
                  </b>
                  <p style="margin-left:0px;">
                    <img src="./images/name.png" , height="60">
                  </p>
                  <p style="margin-left:0px;">
                    <a href="http://data-science.ustc.edu.cn/" , target="_blank">Lab for Data Science</a>
                    <br />
<!--                     <a href="https://eeis.ustc.edu.cn/" , target="_blank">Department of Electronic Engineering and Information Science</a> -->
                    <a href="https://sist.ustc.edu.cn/main.htm" , target="_blank">School of Information Science and Technology</a>
                    <br />
                    <a href="https://ustc.edu.cn/" , target="_blank">University of Science and Technology of China</a>
                    <br />
                  </p>
            <!--       <p style="margin-left:0px;">No.96, JinZhai Road, Baohe District, Hefei, Anhui, 230026, P.R.China. 230027
                    <br></p> -->
<!--                   <p style="margin-left:0px;">Email: shuowang.edu@gmail.com</p> -->
                  <p style="margin-left:0px;">Email: shuowangcv@ustc.edu.cn</p>
                    <div class="social-links">
                        <a href="https://github.com/ShuoWangCS">GitHub</a>
                        <a href="https://scholar.google.com/citations?user=qTE3BacAAAAJ&hl=zh-CN">Google Scholar</a>
                    </div>
                    <br>
                  </p>
                </font>
<!--                 <p>
                  <font size="+0">
                  </font>
                </p> -->
              </td>
            </tr>
          </tbody>
        </table>
        
        <div style="margin-top:20px;">
          <div class="justified-text">
            Hello, I’m Shuo Wang! I am currently an Associate Research Fellow, School of Information Science and Technology, 
            <a href="https://www.ustc.edu.cn/"> University of Science and Technology of China (USTC)</a>, China. My research interests primarily revolve around multimodal content analysis, model lightweighting, and multimodal large models. I focus on the development of efficient methods for processing and analyzing complex multimedia data, with applications in large-scale multimedia retrieval, data embedding, and advanced video understanding. 
          </div>
        </div>

<div id="papers">
    <h2 style="CLEAR: both">Selected Publications</h2>

    <!-- 2025 Papers -->
<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2501.19243" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Accelerating Diffusion Transformer via Error-Optimized Cache</span>
                    <br>
                    Junxiang Qiu, <b>Shuo Wang*</b>, Jinda Lu, Lin Liu, Houcheng Jiang, Yanbin Hao
                    <br>ACM MM, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2503.05156" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Accelerating Diffusion Transformer via Gradient-Optimized Cache</span>
                    <br>
                    Junxiang Qiu, Lin Liu, <b>Shuo Wang*</b>, Jinda Lu, Kezhou Chen, Yanbin Hao
                    <br>ICCV, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>


<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2507.03657" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Dynamic Multimodal Prototype Learning in Vision-Language Models</span>
                    <br>
                    Xingyu Zhu, <b>Shuo Wang*</b>, Beier Zhu, Miaoge Li, Yunfan Li, Junfeng Fang, Zhicai Wang, Dongsheng Wang, Hanwang Zhang
                    <br>ICCV, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://ieeexplore.ieee.org/document/10814067" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Symmetric Hallucination with Knowledge Transfer for Few-shot Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Xinyu Zhang, Meng Wang, Xiangnan He
                    <br>IEEE Transactions on Multimedia, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2410.19294" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting</span>
                    <br>
                    Xingyu Zhu, Beier Zhu, Yi Tan, <b>Shuo Wang*</b>, Yanbin Hao, Hanwang Zhang
                    <br>NeurIPS (Spotlight), 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2407.16977" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Selective Vision-Language Subspace Projection for Few-shot CLIP</span>
                    <br>
                    Xingyu Zhu, Beier Zhu, Yi Tan, <b>Shuo Wang*</b>, Yanbin Hao, Hanwang Zhang
                    <br>ACM MM, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<!-- <table><tbody><tr><td class="left">
                  <a href="#" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Rethinking Visual Content Refinement in Low-Shot CLIP Adaptation</span>
                    <br>
                    Jinda Lu, <b>Shuo Wang*</b>, Yanbin Hao, Haifeng Liu, Xiang Wang, Meng Wang
                    <br>arXiv preprint arXiv:2407.14117, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
     -->
<table><tbody><tr><td class="left">
                    <a href="https://www.researchgate.net/publication/381923178_Feature_Mixture_on_Pre-trained_Model_for_Few-shot_Learning" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Feature Mixture on Pre-Trained Model for Few-Shot Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Jinda Lu, Haiyang Xu, Yanbin Hao, Xiangnan He
                    <br>IEEE Transactions on Image Processing, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2403.17025" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Boosting Few-Shot Learning via Attentive Feature Regularization</span>
                    <br>
                    Xingyu Zhu, <b>Shuo Wang*</b>, Jinda Lu, Yanbin Hao, Haifeng Liu, Xiangnan He
                    <br>AAAI, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    

    
<table><tbody><tr><td class="left">
                    <a href="https://hexiangnan.github.io/papers/mm23-4S-few-shot.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Semantic-based Selection, Synthesis, and Supervision for Few-shot Learning</span>
                    <br>
                    Jinda Lu, <b>Shuo Wang*</b>, Xinyu Zhang, Yanbin Hao, Xiangnan He*
                    <br>ACM MM, 2023&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://ieeexplore.ieee.org/document/9952204" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Spatio-Temporal Collaborative Module for Efficient Action Recognition</span>
                    <br>
                    Yanbin Hao, <b>Shuo Wang*</b>, Yi Tan, Xiangnan He, Zhenguang Liu, Meng Wang
                    <br>IEEE Transactions on Image Processing, 2022&nbsp;&nbsp;</td>
</tr></tbody></table>
    

<table><tbody><tr><td class="left">
                    <a href="https://hexiangnan.github.io/papers/mm22-knowledge-few-shot.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Multi-directional Knowledge Transfer for Few-shot Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Xinyu Zhang, Yanbin Hao, Chengbing Wang, Xiangnan He
                    <br>ACM MM, 2022&nbsp;&nbsp;</td>
</tr></tbody></table>
    

    
<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2204.09303" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Attention in Attention: Modeling Context Correlation for Efficient Video Classification</span>
                    <br>
                    Yanbin Hao, <b>Shuo Wang*</b>, Pei Cao, Xinjian Gao, Tong Xu, Jinmeng Wu, Xiangnan He
                    <br>IEEE Transactions on Circuits and Systems for Video Technology, 2022&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550715.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Large-scale Few-shot Learning via Multi-modal Knowledge Discovery</span>
                    <br>
                    <b>Shuo Wang</b>, Jun Yue, Jianzhuang Liu, Qi Tian, Meng Wang
                    <br>ECCV, 2020&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<!-- <table><tbody><tr><td class="left">
                    <a href="https://www.ijcai.org/proceedings/2019/105" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Dense Temporal Convolution Network for Sign Language Translation</span>
                    <br>
                    Dan Guo, <b>Shuo Wang</b>, Qi Tian, Meng Wang
                    <br>IJCAI, 2019&nbsp;&nbsp;</td>
</tr></tbody></table> -->
    
<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3240508.3240671" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Connectionist Temporal Fusion for Sign Language Translation</span>
                    <br>
                    <b>Shuo Wang</b>, Dan Guo, Wengang Zhou, Zhengjun Zha, Meng Wang
                    <br>ACM MM, 2018&nbsp;&nbsp;</td>
</tr></tbody></table>
    

<table><tbody><tr><td class="left">
                    <a href="#" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Method and Apparatus for Training Classifier</span>
                    <br>
                    <b>Shuo Wang</b>, Jun Yue, Jianzhuang Liu, Qi Tian
                    <br>US Patent App. 17/892,908, 2023&nbsp;&nbsp;</td>
</tr></tbody></table>


<!-- <div id="other papers"> -->
<h2 style="CLEAR: both">Other Publications (Five years)</h2>
  
<table><tbody><tr><td class="left">
                    <a href="" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Cross-Modal Feature Enhancement and Contrastive Alignment for Micro-Gesture Recognition</span>
                    <br>
                    Tuyun Shang, Yanbin Hao, Ming Pei, Kun Li, Huixia Ben, <b>Shuo Wang</b> 
                    <br>The 8th Chinese Conference on Pattern Recognition and Computer Vision, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3729171" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Interventional Feature Generation for Few-shot Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Jinda Lu, Huixia Ben, Yanbin Hao, Xingyu Gao, Meng Wang
                    <br>ACM Transactions on Multimedia Computing, Communications and Applications 2025, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://aclanthology.org/2025.findings-naacl.31/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Multimodal Generation with Consistency Transferring</span>
                    <br>
                    Junxiang Qiu, Jinda Lu, <b>Shuo Wang</b>
                    <br>Findings of the Association for Computational Linguistics: NAACL 2025, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://aclanthology.org/2025.naacl-long.90/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Mixture of Multimodal Adapters for Sentiment Analysis</span>
                    <br>
                    Kezhou Chen, Huixia Ben, <b>Shuo Wang</b>, Shengeng Tang, Yanbin Hao
                    <br>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2412.16944" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Linguistics-Vision Monotonic Consistent Network for Sign Language Production</span>
                    <br>
                    Xu Wang, Shengeng Tang, Peipei Song, <b>Shuo Wang</b>, Dan Guo, Richang Hong
                    <br>International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3663572" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Gloss-Driven Conditional Diffusion Models for Sign Language Production</span>
                    <br>
                    Shengeng Tang, Feng Xue, Jingjing Wu, <b>Shuo Wang</b>, Richang Hong
                    <br>ACM Transactions on Multimedia Computing, Communications and Applications, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2502.01943" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">DAMO: Data-and Model-aware Alignment of Multi-modal LLMs</span>
                    <br>
                    Jinda Lu, Junkang Wu, Jinghan Li, Xiaojun Jia, <b>Shuo Wang</b> YiFan Zhang, Junfeng Fang, Xiang Wang, Xiangnan He
                    <br>ICML 2025, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3404835.3462874" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Video Corpus Moment Retrieval with Query-specific Context Learning and Progressive Localization</span>
                    <br>
                    Long Zhang, Peipei Song, Zhangling Duan, <b>Shuo Wang</b>, Xiaojun Chang, Xun Yang
                    <br>TCSVT 2025, 2025&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://www.researchgate.net/publication/387288956_CVLP-NaVD_Contrastive_Visual-Language_Pre-training_Models_for_Non-annotated_Visual_Description" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">CVLP-NaVD: Contrastive Visual-Language Pre-training Models for Non-annotated Visual Description</span>
                    <br>
                    Haoran Li, Yanbin Hao, Jiarui Yu, Bin Zhu, <b>Shuo Wang</b>, Tong Xu
                    <br>ACM Transactions on Multimedia Computing, Communications and Applications, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3652583.3658080" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Pseudo Content Hallucination for Unpaired Image Captioning</span>
                    <br>
                    Huixia Ben, <b>Shuo Wang*</b>, Meng Wang, Richang Hong
                    <br>Proceedings of the 2024 International Conference on Multimedia Retrieval, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2312.04763" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective</span>
                    <br>
                    Fangzhou Song, Bin Zhu, Yanbin Hao, <b>Shuo Wang</b>
                    <br>European Conference on Computer Vision, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-97-8508-7_9" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">JPA: A Joint-Part Attention for Mitigating Overfocusing on 3D Human Pose Estimation</span>
                    <br>
                    Dengqing Yang, Zhenhua Tang, Jinmeng Wu, <b>Shuo Wang</b>, Lechao Cheng, Yanbin Hao
                    <br>Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2024&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://bmvc2024.org/proceedings/425/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">GLCM-Adapter: Global-Local Content Matching for Few-shot CLIP Adaptation</span>
                    <br>
                    <b>Shuo Wang</b>, Enlong Xie, Jinda Lu, Jinghan Li, Yanbin Hao
                    <br>Proceedings of the 35th British Machine Vision Conference (BMVC), 2024&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-53308-2_5" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Hierarchical Supervised Contrastive Learning for Multimodal Sentiment Analysis</span>
                    <br>
                    Kezhou Chen, <b>Shuo Wang*</b>, Yanbin Hao
                    <br>International Conference on Multimedia Modeling, 2024&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://papers.bmvc2023.org/0367.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">How Can Contrastive Pre-training Benefit Audio-visual Segmentation? A Study from Supervised and Zero-shot Perspectives</span>
                    <br>
                    Jiarui Yu, Haoran Li, Yanbin Hao, Jinmeng Wu, Tong Xu, <b>Shuo Wang</b>, Xiangnan He
                    <br>Proceedings of the 34th British Machine Vision Conference (BMVC), 2023&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2303.08698" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Bi-directional Distribution Alignment for Transductive Zero-Shot Learning</span>
                    <br>
                    Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, <b>Shuo Wang</b>, Xiangnan He
                    <br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3522713" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Boosting Hyperspectral Image Classification with Dual Hierarchical Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Huixia Ben, Yanbin Hao, Xiangnan He, Meng Wang
                    <br>ACM Transactions on Multimedia Computing, Communications and Applications, 2023&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://dl.acm.org/doi/10.1145/3503161.3547841" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Hierarchical Hourglass Convolutional Network for Efficient Video Classification</span>
                    <br>
                    Yi Tan, Yanbin Hao, Hao Zhang, <b>Shuo Wang</b>, Xiangnan He
                    <br>Proceedings of the 30th ACM International Conference on Multimedia, 2022&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://arxiv.org/abs/2207.07284" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP</span>
                    <br>
                    Zhicai Wang, Yanbin Hao, Xingyu Gao, Hao Zhang, <b>Shuo Wang</b>, Tingting Mu, Xiangnan He
                    <br>ACM MM, 2022&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://ieeexplore.ieee.org/document/9853253/" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">IPFC: An Attentive Face Completion Network with Identity Preserving</span>
                    <br>
                    Xin Ni, Haiyong Xie, Yuyan Yang, <b>Shuo Wang</b>, Wenshan Wang, Yifeng Liu
                    <br>2022 International Symposium on Electrical, Electronics and Information Engineering (ISEEIE), 2022&nbsp;&nbsp;</td>
</tr></tbody></table>

<table><tbody><tr><td class="left">
                    <a href="https://iopscience.iop.org/article/10.1088/1742-6596/2024/1/012063" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Space-time Separate Modeling for Efficient Video Classification</span>
                    <br>
                    Pei Cao, <b>Shuo Wang*</b>, Jinmeng Wu, Yanbin Hao
                    <br>Journal of Physics: Conference Series, 2021&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                    <a href="https://dspace.ustc.edu.cn/wp-content/uploads/2021/10/Thinking-in-Patch-Towards-Generalizable-Forgery-Detection-with-Patch-Transformation.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Thinking in Patch: Towards Generalizable Forgery Detection with Patch Transformation</span>
                    <br>
                    Xueqi Zhang, <b>Shuo Wang</b>, Chenyu Liu, Min Zhang, Xiaohan Liu, Haiyong Xie
                    <br>PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8–12, 2021, Proceedings, Part III 18, 2021&nbsp;&nbsp;</td>
</tr></tbody></table>

<!-- <table><tbody><tr><td class="left">
                  <a href="#" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">Cross-modality Retrieval by Joint Correlation Learning</span>
                    <br>
                    <b>Shuo Wang</b>, Dan Guo, Xin Xu, Li Zhuo, Meng Wang
                    <br>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2019&nbsp;&nbsp;</td>
</tr></tbody></table>
    
<table><tbody><tr><td class="left">
                  <a href="#" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>Paper</a></td><td>
                    <span class="title">基于 Kinect 3D 节点的连续 HMM 手语识别</span>
                    <br>
                    沈娟; 王硕; 郭丹;
                    <br>合肥工业大学学报: 自然科学版, 2017&nbsp;&nbsp;</td>
</tr></tbody></table>
 -->

<h2 style="CLEAR: both">More Publications See <a href="https://scholar.google.com/citations?user=qTE3BacAAAAJ&hl=zh-CN">Google Scholar</a></h2>
<br>


<h2 style="clear: both">Grants</h2>
<table class="grants-table">
      <tr>
        <td><span class="title">国家自然科学基金面上项目</span></td>
        <td>项目负责人</td>
        <td>2026.01-2029.12</td>
    </tr>
      <tr>
        <td><span class="title">安徽省自然科学基金面上项目</span></td>
        <td>项目负责人</td>
        <td>2025.09-2028.09</td>
    </tr>
  
    <tr>
        <td><span class="title">长三角科技创新共同体联合攻关项目</span></td>
        <td>课题负责人兼课题参与人</td>
        <td>2024.12-2027.11</td>
    </tr>
    <tr>
        <td><span class="title">国家自然科学基金青年科学基金项目（C类）</span></td>
        <td>项目负责人</td>
        <td>2023.01-2024.12</td>
    </tr>
    <tr>
        <td><span class="title">安徽高校协同创新项目</span></td>
        <td>联合牵头负责人兼课题负责人</td>
        <td>2021.08-2023.08</td>
    </tr>
    <tr>
        <td><span class="title">JKW 国防科技创新项目</span></td>
        <td>课题负责人</td>
        <td>2020.12-2023.08</td>
    </tr>
</table>

<h2 style="clear: both">Professional Services</h2>
<ul class="service-list">
    <li>中国计算机学会(CCF)多媒体技术专业委员会执行委员</li>
    <li>中国中⽂信息学会社会媒体处理专委会委员</li>
    <li>Guest Editor of <a href="https://www.mdpi.com/journal/electronics" target="_blank">Electronics</a></li>
    <li>Reviewer for CVPR, ICCV, ECCV, NeurIPS, AAAI, IJCAI, MM, TPAMI, TIP, TNNLS, TCSVT, PR, Neurocomputing, Information Sciences. et al</li>
</ul>

<h2 style="CLEAR: both">Education and Experiences</h2>
<table>
  <tbody><tr><td><span class="title">University of Science and Technology of China (USTC)</span><br>Postdoc Research Fellow &nbsp;&nbsp;&nbsp;&nbsp; Mar. 2021 - Mar. 2023, Hefei, Anhui, China<br>Advisor: Prof. <a href="https://hexiangnan.github.io/" target="_blank"><strong>Xiangnan He</strong></a></a><br></td></tr></tbody>
<!--     Postdoc Research Fellow</span>, University of Science and Technology of China <br> Advisor: Prof. <a href="https://hexiangnan.github.io/" target="_blank"><strong>Xiangnan He</strong></a>,  &nbsp;&nbsp;&nbsp;&nbsp; Mar. 2021 - Mar. 2023 <br></td></tr></tbody> -->
<!--   <tbody><tr><td> <span class="title">Postdoc Research Fellow</span>, University of Science and Technology of China <br> Advisor: Prof. <a href="https://hexiangnan.github.io/" target="_blank"><strong>Xiangnan He</strong></a>,  &nbsp;&nbsp;&nbsp;&nbsp; Mar. 2021 - Mar. 2023 <br></td></tr></tbody> -->
  <tbody><tr><td><span class="title">Hefei University of Technology (HFUT)</span><br>Ph.D. Student of Signal and Information Processing &nbsp;&nbsp;&nbsp;&nbsp; Sep. 2015 - Jan. 2021, Hefei, Anhui, China<br>Advisor: Prof. <a href="http://ci.hfut.edu.cn/7896/listm.htm" target="_blank"><strong>Meng Wang</strong></a></a><br></td></tr></tbody>
  <tbody><tr><td><span class="title">Hefei University of Technology (HFUT)</span><br>Bachelor"s Degree in Electronics Engineering &nbsp;&nbsp;&nbsp;&nbsp; Sep. 2011 - Jun. 2015, Hefei, Anhui, China<br>Advisor: Prof. <a href="http://ci.hfut.edu.cn/7896/listm.htm" target="_blank"><strong>Meng Wang</strong></a></a><br></td></tr></tbody>
</table>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=8REUb4OI99lfOHCOlkoQeuZAPSmKeKC4v-GMsgSYtxs&cl=ffffff&w=a"></script>

          <p>Last update: 30 Aug. 2025. The webpage template borrows from <a href="http://staff.ustc.edu.cn/~hexn/">Xiangnan He</a>.</p>
        </div>
      </div>
  </body>

</html>
