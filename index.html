<!DOCTYPE html>
<html>
  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="IE=7.0000" http-equiv="X-UA-Compatible">
    <title>Shuo Wang's Homepage</title>
    <meta name="description" content="Shuo Wang, Postdoc in University of Science and Technology of China. Recommender systems, information retrieval, graph learning, contrastive learning, causal inference.">
    <meta name="keywords" content="Shuo Wang, USTC, University of Science and Technology of China, recommender systems, personalized recommendation, information retrieval, deep learning, machine learning, graph learning, self-supervised learning">
    <link rel="stylesheet" type="text/css" href="./files/ShuoWang.css">
    <style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style>
  </head>
  
  <body>
    <div id="content">
      <div id="news">
        <h2>News</h2>
        <br>
        <font size="3px">
          <b>15 Sep 2023</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://bmvc2023.org/" target="_blank">  BMVC 2023</a>, about
            <b>How Can Contrastive Pre-training Benefit Audio-Visual Segmentation? A Study from Supervised and Zero-shot Perspectives</b>.</span>
          <br>
          <br>
          <b>15 July 2023</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://www.acmmm2023.org/" target="_blank">  MM 2023</a>, about
            <b>Semantic-based Selection, Synthesis, and Supervision for Few-shot Learning</b>.</span>
          <br>
          <br>
          <b>15 June 2023</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://cvpr2023.thecvf.com/Conferences/2023" target="_blank">  CVPR 2023</a>, about
            <b>Bi-directional Distribution Alignment for Transductive Zero-Shot Learning</b>.</span>
          <br>
          <br>
          <b>15 Jan 2023</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://dl.acm.org/journal/tomm" target="_blank">TOMM 2023</a>, about
            <b>Boosting Hyperspectral Image Classification with Dual Hierarchical Learning</b>.</span>
          <br>
          <br>
          <b>15 Nov 2022</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">TIP 2022</a>, about
            <b>Spatio-Temporal Collaborative Module for Efficient Action Recognition</b>.</span>
          <br>
          <br>
          <b>10 Oct 2022</b>
          <br>
          <span class="easylink">Three papers is accepted by
            <a href="https://2022.acmmm.org/" , target="_blank">ACM MM 2022</a>on
            <b>few-shot learning, video classification and vision structure.</b>.</span>
          <br>
          <br>
          <b>22 Apr 2022</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://sigir.org/sigir2021/" target="_blank">TCSVT 2022</a>, about
            <b>Attention in attention: Modeling context correlation for efficient video classification</b>.</span>
          <br>
          <br>
          <b>25 Feb 2022</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://www.iseeie.org/" target="_blank">ISEEIE 2022</a>, about
            <b>IPFC: An Attentive Face Completion Network with Identity Preserving</b>.</span>
          <br>
          <br>
          <b>18 Oct 2021</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://iopscience.iop.org/year/1742-6596/Y2021" target="_blank">Journal of Physics 2021</a>, about
            <b>Space-Time Separate Modeling for Efficient Video Classification</b>.</span>
          <br>
          <br>
          <b>10 Oct 2021</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://www.pricai.org/2021/" target="_blank">PRICAI 2021</a>, about
            <b>Thinking in patch: Towards generalizable forgery detection with patch transformation</b>.</span>
          <br>
          <br>
          <b>3 Jul 2021</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>, about
            <b>Large-scale few-shot learning via multi-modal knowledge discovery</b>.</span>
          <br>
          <br>
          <b>10 May 2019</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://www.ijcai19.org/" target="_blank">IJCAI 2019</a>, about
            <b>Dense Temporal Convolution Network for Sign Language Translation</b>.</span>
          <br>
          <br>
          <b>30 Sep 2019</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="https://dl.acm.org/journal/tomm" target="_blank">TOMM 2019</a>, about
            <b>Cross-modality retrieval by joint correlation learning</b>.</span>
          <br>
          <br>
          <!-- <b>19 Sep 2018</b>
          <br>
          <span class="easylink">One paper is accepted by
            <a href="http://www.acmmm.org/2018/" target="_blank">ACM MM 2018</a>, about
            <b>Connectionist temporal fusion for sign language translation</b>.</span>
          <br>
          <br> -->
        </font>
      </div>
      <div id="left">
        <table style="background-color:white;">
          <tbody>
            <tr nosave="">
              <td valign="CENTER">
                <img src="./images/profile.jpg" height="250" align="left">
              </td>
              <td valign="CENTER" width="2%">
              </td>
              <td valign="CENTER" halign="LEFT">
                <font size="+0">
                  <b>
                    <font size="+2">Shuo Wang&nbsp;</font>
                  </b>
                  <p style="margin-left:0px;">
                    <img src="./images/name.png" , height="60">
                  </p>
                  <p style="margin-left:0px;">
                    <a href="http://data-science.ustc.edu.cn/" , target="_blank">Lab for Data Science</a>
                    <br />
                    <a href="https://eeis.ustc.edu.cn/" , target="_blank">Department of Electronic Engineering and Information Science</a>
                    <br />
                    <a href="https://ustc.edu.cn/" , target="_blank">University of Science and Technology of China</a>
                    <br />
                  </p>
                  <p style="margin-left:0px;">443 Huangshan Road, Hefei, China 230027
                    <br></p>
                  <p style="margin-left:0px;">Email: shuowang.edu AT gmail.com</a>
                    <br>&bull;
                    <a href="https://github.com/ShuoWangCS">GitHub</a>&bull;
                    <a href="https://scholar.google.com/citations?user=qTE3BacAAAAJ&hl=zh-CN">Google Scholar</a>
                    <br>
                  </p>
                </font>
                <p>
                  <font size="+0">
                  </font>
                </p>
              </td>
            </tr>
          </tbody>
        </table>
        <div style="margin-top:20px;">Hello, Iâ€™m Shuo Wang! I am currently a Associate Research Fellow, School of Data Science,
          <a href="https://www.ustc.edu.cn/">University of Science and Technology of China (USTC)</a>, China. His reseach interests mainly include machine learning and multimedia data analysis, such as large-scale multimedia indexing and retrieval, multimedia data embedding, and video understanding. He serves as a technical program committee (TPC) member for ACM MM, CVPR, ECCV, IJCAI, etc, and reviewer for TIP, TMM, TOMM, etc.</div>
        <div id="papers">
          <h2 style="CLEAR: both">Selected Publications</h2>
          </br>



          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=qTE3BacAAAAJ&sortby=pubdate&citation_for_view=qTE3BacAAAAJ:_FxGoFyzp5QC" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Semantic-based Selection, Synthesis, and Supervision for Few-shot Learning</span>
                  <br>
                  Jinda Lu, <b>Shuo Wang</b>, Xinyu Zhang, Yanbin Hao, Xiangnan He
                  <br>CVPR 2023&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=qTE3BacAAAAJ&sortby=pubdate&citation_for_view=qTE3BacAAAAJ:eQOLeE2rZwMC" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Bi-directional Distribution Alignment for Transductive Zero-Shot Learning</span>
                  <br>
                  Zhicai Wang, Yanbin Hao, Tinging Mu, Ouxiang Li, <b>Shuo Wang</b>, Xiangnan He
                  <br>CVPR 2023&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://drive.google.com/file/u/0/d/1iyM_BAFR_RLE-26nnJfpT8__N2hEYCHH/view" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Boosting Hyperspectral Image Classification with Dual Hierarchical Learning</span>
                  <br>
                  <b>Shuo Wang</b>, Huixia Ben, Yanbin Hao, Xiangnan He, Meng Wang
                  <br>TOMM 2023&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9952204" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Spatio-Temporal Collaborative Module for Efficient Action Recognition</span>
                  <br>Yanbin Hao,
                  <b>Shuo Wang</b>, Yi Tan, Xiangnan He, Zhenguang Liu, Meng Wang
                  <br>TIP 2022&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://hexiangnan.github.io/papers/mm22-hourglass-cnn-video.pdf" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Hierarchical Hourglass Convolutional Network for Efficient Video Classification</span>
                  <br>Yi Tan, Yanbin Hao, Hao Zhang,
                  <b>Shuo Wang</b>, Xiangnan He
                  <br>ACM MM 2022&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="http://staff.ustc.edu.cn/~hexn/papers/mm22-knowledge-few-shot.pdf" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Multi-directional Knowledge Transfer for Few-Shot Learning</span>
                  <br>
                  <b>Shuo Wang</b>, Xinyu Zhang, Yanbin Hao, Chengbing Wang, Xiangnan He
                  <br>ACM MM 2022&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://arxiv.org/pdf/2207.07284" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP</span>
                  <br>Zhicai Wang, Yanbin Hao, Xingyu Gao, Hao Zhang,
                  <b>Shuo Wang</b>, Tingting Mu, Xiangnan He
                  <br>ACM MM 2022&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://arxiv.org/pdf/2204.09303" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Attention in attention: Modeling context correlation for efficient video classification</span>
                  <br>Yanbin Hao,
                  <b>Shuo Wang</b>, Pei Cao, Xinjian Gao, Tong Xu, Jinmeng Wu, Xiangnan He
                  <br>TCSVT 2022&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://drive.google.com/file/d/1oCWL_DzofQqUc5nDtn7N2RZ942Mfj_o5/view" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Large-scale few-shot learning via multi-modal knowledge discovery</span>
                  <br>
                  <b>Shuo Wang</b>, Jun Yue, Jianzhuang Liu, Qi Tian, Meng Wang
                  <br>ECCV 2020&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://www.ijcai.org/proceedings/2019/0105.pdf" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Dense Temporal Convolution Network for Sign Language Translation</span>
                  <br>Dan Guo,
                  <b>Shuo Wang</b>, Qi Tian, and Meng Wang
                  <br>IJCAI 2019&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://drive.google.com/file/d/1tbuysMu54br6_Ip76RArjrEP3_q6QEo4/view" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Cross-modality retrieval by joint correlation learning</span>
                  <br>
                  <b>Shuo Wang</b>, Dan Guo, Xin Xu, Li Zhuo, and Meng Wang
                  <br>TOMM 2019&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td class="left">
                  <a href="https://tangshengeng.github.io/VUT-homepage/publications/ACMMM2018CTF/paper.pdf" target="_blank">
                    <img src="./images/pdf.png" width="25" height="25">
                    <br>pdf</a>
                </td>
                <td>
                  <span class="title">Connectionist temporal fusion for sign language translation</span>
                  <br>
                  <b>Shuo Wang</b>, Dan Guo, Wen-gang Zhou, Zheng-Jun Zha, and Meng Wang
                  <br>ACM MM 2018&nbsp;&nbsp;</td>
              </tr>
            </tbody>
          </table>
          <h2 style="CLEAR: both;">Education</h2>
          <table>
            <tbody>
              <tr>
                <td>
                  <span class="title">University of Science and Technology of China(USTC)</span>
                  <br>Ph.D. Student of Signal and Information Processing &nbsp;&nbsp;&nbsp;&nbsp; Sep 2015 - Jan 2021, Hefei, Anhui, China
                  <br>Advisor: Prof.
                  <a href="http://ci.hfut.edu.cn/2018/0821/c8015a168776/page.htm" target="_blank">
                    <strong>Dan Guo</strong>
                  </a>
                  </a>and Prof.
                  <a href="http://ci.hfut.edu.cn/7896/listm.htm" target="_blank">
                    <strong>Meng Wang</strong>
                  </a>
                  </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>
          <table>
            <tbody>
              <tr>
                <td>
                  <span class="title">University of Science and Technology of China(USTC)</span>
                  <br>Bachelor"s Degree in Electronics Engineering &nbsp;&nbsp;&nbsp;&nbsp; Sep 2011 - June 2015, Hefei, Anhui, China
                  <br>Advisor: Prof.
                  <a href="http://ci.hfut.edu.cn/7940/listm.htm" target="_blank">
                    <strong>XueLiang Liu</strong>
                  </a>and Prof.
                  <a href="http://ci.hfut.edu.cn/7896/listm.htm" target="_blank">
                    <strong>Meng Wang</strong>
                  </a>
                  </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <h2 style="CLEAR: both;">Useful Links</h2> -->
 <!--          <table>
            <tbody>
              <tr>
                <td>
                  <span class="title">
                    <a href="https://docs.google.com/document/d/1IXF3h0RU5zz4ukmTrVKVotPQypChscNGf5k6E25HGvA" target="_blank">Machine Learning Reading List</a>
                  </span>
                </td>
              </tr>
              <tr>
                <td>
                  <span class="title">
                    <a href="https://github.com/robertsdionne/neural-network-papers" target="_blank">Deep Learning Reading List</a>
                  </span>
                </td>
              </tr>
            </tbody>
          </table> -->
          </br>
          <p>Last update: Mar 13, 2023. Webpage template borrows from
            <a href="http://staff.ustc.edu.cn/~hexn/">Xiangnan He</a>.</p>
        </div>
      </div>
  </body>

</html>
